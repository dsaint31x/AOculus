{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_DecisionTree.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB2hpzVZfDsUN3RuXfKj+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsaint31x/AOculus/blob/master/ML_DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePzBYug8Lo9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-JIABFcLrOa",
        "colab_type": "text"
      },
      "source": [
        "## The CART Training Alogrithm\n",
        "\n",
        "Scikit-Learn uses the *Classification And Regression Tree* (**CART**) alogrithms to train Decision Trees (also called \"growing\" tress). \n",
        "\n",
        "The algorithm works by first spliting the training set into two subsets using a single feature $k$ and a threshold $t_k$ (e.g., \"petal length $\\le$ 2.45cm\").\n",
        "\n",
        "How dose it choose $k$ and $t_k$?\n",
        "\n",
        "It searches for the pair $(k,t_k)$ that **produces the pure subsets (weighted by their size).**\n",
        "\n",
        "Equation 6-2 gives the cost function that the algorithm tries to minimize.\n",
        "\n",
        "### Equation 6-2. CART cost function for classification\n",
        "\n",
        "$$\n",
        "J(k,t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
        "$$\n",
        "\n",
        "where\n",
        "* $G_{left/right}$ measures the impurity of the left/right subset,\n",
        "* $m_{left/right}$ is the number of instances in the left/right subset.\n",
        "\n",
        "Once the CART algorithm has successfully split into the training set in two, it split the subsets using the same logic, then the sub-subsets, and so on, recursively. \n",
        "\n",
        "It stops recursing once it reaches the maximum depth (defined by the `max_depth` hyperparameter), or if it cannot find a split that will reduce impurity. \n",
        "\n",
        "A few other hyperparmeters (described in a moment) control additional stopping conditions (`min_samples_split`,`min_samples_leaf`, `min_weight_fraction_leaf`, and `max_leaf_nodes`)\n",
        "\n",
        "\n",
        "> ### WARNING\n",
        ">\n",
        "> As you can see, the CART algorithm is a *greedy algorithm*: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.\n",
        "> \n",
        "> Unfortunately, finding the optimal tree is known to be an *NP-Complete problem*: it requires $O(\\exp(m))$ time, making the problem intractable even for small training sets. **This is why we must settle for a “reasonably good” solution.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEed-3tZPQEV",
        "colab_type": "text"
      },
      "source": [
        "## Computational Complexity\n",
        "\n",
        "Making predictions requires traversing the Decision Tree from the root to a leaf. \n",
        "\n",
        "Decision Trees generally are approximately balanced, so traversing the Decision Tree requires going through roughly $O(\\log_2(m))$ nodes. \n",
        "\n",
        "Since each node only requires checking the value of one feature, the overall prediction complexity is $O(\\log_2(m))$, **independent of the number of features**. So predictions are very fast, even when dealing with large training sets.\n",
        "\n",
        "\n",
        "The training algorithm compares all features (or less if `max_features` is set) on all samples at each node. Comparing all features on all samples at each node results in a training complexity of $O(n \\times m \\log_2(m))$. \n",
        "\n",
        "For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set `presort=True`), but doing that slows down training considerably for larger training sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRzxILK8Qt4J",
        "colab_type": "text"
      },
      "source": [
        "## Gini Impurity or Entropy?\n",
        "\n",
        "By default, the Gini impurity measure is used, but you can select the **entropy** impurity measure instead by setting the criterion hyperparameter to \"`entropy`\". \n",
        "\n",
        "The concept of entropy originated in thermodynamics as a measure of molecular disorder: `entropy` **approaches zero when molecules are still and well ordered**. Entropy later spread to a wide variety of domains, including Shannon’s information theory, where it measures the average information content of a message: `entropy` is zero when all messages are identical. \n",
        "In Machine Learning, `entropy` is frequently used as **an impurity measure**: a set’s entropy is zero when it contains instances of only one class. \n",
        "\n",
        "Equation 6-3 shows the definition of the entropy of the i-th node. For example, the depth-2 left node in Figure 6-1 has an entropy equal to –$(49/54) \\log_2 (49/54) – (5/54) \\log_2 (5/54) \\approx 0.445$.\n",
        "\n",
        "\n",
        "### Equation 6-3. Entropy\n",
        "\n",
        "$$\n",
        "H_i = - \\sum^n _{k=1\\\\p_{i,k} \\ne 0} p_{i,k} \\log_2 (p_{i,k})\n",
        "$$\n",
        "\n",
        "So, should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. \n",
        "\n",
        "* **Gini impurity is slightly faster to compute**, so it is a good default. However, when they differ, \n",
        "* **Gini impurity tends to isolate the most frequent class in its own branch of the tree**, while \n",
        "* **entropy tends to produce slightly more balanced trees**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd2Gq6YfCvZ8",
        "colab_type": "text"
      },
      "source": [
        "## Regularization Hyperparameters\n",
        "\n",
        "Decision Trees make very few assumptions about the training data (as opposed to linear models, which assume that the data is linear, for example). If left **unconstrained**, the tree structure will adapt itself to the training data, fitting it very closely—indeed, most likely **overfitting** it. Such a model is often called a **nonparametric model**, not because it does not have any parameters (it often has a lot) but **because the number of parameters is not determined prior to training**, so the model structure is free to stick closely to the data. \n",
        "\n",
        "In contrast, *a **parametric model**, such as a linear model*, has a **predetermined number of parameters**, so **its degree of freedom is limited**, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
        "\n",
        "**To avoid overfitting** the training data, you need to **restrict the Decision Tree’s freedom during training**. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
        "\n",
        "The `DecisionTreeClassifier` class has a few other parameters that similarly restrict the shape of the Decision Tree: \n",
        "* `min_samples_split` (the minimum number of samples a node must have before it can be split), \n",
        "* `min_samples_leaf` (the minimum number of samples a leaf node must have), \n",
        "* `min_weight_fraction_leaf` (same as min_samples_leaf but expressed as a fraction of the total number of weighted instances), \n",
        "* `max_leaf_nodes` (the maximum number of leaf nodes), and \n",
        "* `max_features` (the maximum number of features that are evaluated for splitting at each node). \n",
        "\n",
        "Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
        "\n",
        ">\n",
        "> ### NOTE\n",
        ">\n",
        "> Other algorithms work by first training the **Decision Tree** without restrictions, then **pruning** (deleting) unnecessary nodes. \n",
        "> A node whose children are all leaf nodes is considered unnecessary if **the purity improvement it provides is *not statistically significant*.** \n",
        ">\n",
        "> Standard statistical tests, such as \n",
        "> * the χ2 test (chi-squared test), \n",
        ">   * are used to estimate the probability that the improvement is purely the result of chance (which is called the null hypothesis). \n",
        ">   * If this probability, called the **p-value**, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. \n",
        "> \n",
        "> The pruning continues until all unnecessary nodes have been pruned.\n",
        "\n",
        "Figure 6-3 shows two Decision Trees trained on the moons dataset (introduced in Chapter 5). On the left the Decision Tree is trained with the default hyperparameters (i.e., no restrictions), and on the right it’s trained with `min_samples_leaf=4`. \n",
        "\n",
        "It is quite obvious that the model on the left is overfitting, and the model on the right will probably generalize better.\n",
        "\n",
        "![mls2_0603.png](./fig/mls2_0603.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}